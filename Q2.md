# Fine-Tuning Strategies for Large Pre-trained Models on the iNaturalist Dataset

When working with large pre-trained models like GoogLeNet, InceptionV3, ResNet50, VGG, EfficientNetV2, or Vision Transformers, the computational cost of fine-tuning can be prohibitive, especially with limited resources. The common trick used to keep training tractable is **freezing** layers. Freezing prevents the weights of certain layers from being updated during training, which significantly reduces memory requirements and training time while still leveraging the powerful feature extraction capabilities of these models.

## Understanding Freezing in Fine-Tuning

Freezing layers is a key technique in transfer learning that works by setting the `requires_grad` parameter to `False` for selected layers, preventing gradient computation and weight updates during backpropagation. This approach is particularly effective because:

1. It reduces the number of trainable parameters, decreasing memory usage by 12-20 times compared to full fine-tuning[^11]
2. It helps prevent catastrophic forgetting, where the model loses its pre-trained knowledge[^9][^11]
3. It can improve generalization, especially with limited training data[^15]

## Fine-Tuning Strategies Implemented on the iNaturalist Dataset

I implemented several freezing strategies to fine-tune pre-trained models on the iNaturalist dataset:

### Strategy 1: Freezing All Layers Except the Last Layer

This approach, often called "last-layer fine-tuning," involves freezing the entire pre-trained model and only training the final classification layer that was replaced to match our 10 classes.

```python
import torch
import torch.nn as nn
import torchvision.models as models

# Load pre-trained ResNet50
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

# Freeze all layers
for param in model.parameters():
    param.requires_grad = False
    
# Replace the final fully connected layer
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)  # 10 classes for iNaturalist
```

This strategy is extremely efficient computationally and works well when the target dataset (iNaturalist) is similar to the source dataset (ImageNet). It's particularly effective when working with limited data, as it prevents overfitting by minimizing the number of trainable parameters[^12][^15].

### Strategy 2: Freezing the Base Layers (Early Layers)

In this approach, I froze the early layers (feature extraction layers) of the network while allowing the later layers to be fine-tuned. For ResNet50, this meant freezing the first few blocks:

```python
# Load pre-trained ResNet50
model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

# Freeze the first 3 blocks (early layers)
ct = 0
for child in model.children():
    ct += 1
    if ct &lt; 7:  # First 6 children modules are frozen
        for param in child.parameters():
            param.requires_grad = False
            
# Replace the final fully connected layer
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)  # 10 classes for iNaturalist
```

This strategy is based on the understanding that early layers in CNNs capture generic features like edges and textures, while deeper layers learn more task-specific features[^13][^17]. By freezing early layers, we preserve the general feature extraction capabilities while allowing the model to adapt its higher-level feature representations to the iNaturalist dataset.

### Strategy 3: Progressive Unfreezing (Gradual Fine-Tuning)

This more sophisticated approach involves starting with all layers frozen except the classification layer, training for a few epochs, then gradually unfreezing more layers from top to bottom:

```python
# Initial phase: train only the classifier
for param in model.parameters():
    param.requires_grad = False
    
# Replace and train the final layer
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, 10)
optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)

# Train for a few epochs...

# Second phase: unfreeze the last block
for param in model.layer4.parameters():  # Unfreeze the last ResNet block
    param.requires_grad = True
    
# Use a lower learning rate for fine-tuning
optimizer = torch.optim.Adam([
    {'params': model.fc.parameters(), 'lr': 0.001},
    {'params': model.layer4.parameters(), 'lr': 0.0001}
])

# Train for more epochs...

# Continue unfreezing more layers gradually if needed
```

This progressive approach helps prevent catastrophic forgetting and allows for more controlled fine-tuning[^14][^17]. By using different learning rates for different layers (lower rates for pre-trained layers), we ensure that the valuable pre-trained weights are only slightly adjusted.

### Strategy 4: Selective Layer Freezing Based on Dataset Size

I also implemented a strategy that adapts the freezing approach based on the size and complexity of the iNaturalist dataset:

```python
# For ResNet50, different freezing strategies based on dataset characteristics
if dataset_size &lt; 10000:  # Small dataset
    # Freeze all except the last layer
    for param in model.parameters():
        param.requires_grad = False
    num_features = model.fc.in_features
    model.fc = nn.Linear(num_features, 10)
    
elif 10000 &lt;= dataset_size &lt; 50000:  # Medium dataset
    # Freeze early layers (first 3 blocks)
    ct = 0
    for child in model.children():
        ct += 1
        if ct &lt; 7:
            for param in child.parameters():
                param.requires_grad = False
                
else:  # Large dataset
    # Only freeze the first block
    for param in model.conv1.parameters():
        param.requires_grad = False
    for param in model.bn1.parameters():
        param.requires_grad = False
    for param in model.layer1.parameters():
        param.requires_grad = False
```

This adaptive approach follows the guidance from the StackOverflow answer[^13], which suggests different freezing strategies based on dataset characteristics. For the iNaturalist dataset with its natural images, this strategy helps balance between leveraging pre-trained features and adapting to the specific characteristics of biological specimens.

## Comparison of Strategies

Each strategy offers different trade-offs between computational efficiency and model performance:

1. **Freezing All Except Last Layer**: Fastest training, lowest memory usage, but potentially lower accuracy if iNaturalist features differ significantly from ImageNet.
2. **Freezing Base Layers**: Good balance between training efficiency and model adaptability, allowing the model to learn dataset-specific high-level features while preserving low-level feature extraction.
3. **Progressive Unfreezing**: Most sophisticated approach with potentially best performance, but requires more careful hyperparameter tuning and slightly more training time.
4. **Selective Layer Freezing**: Adaptable approach that can be tailored to the specific characteristics of the dataset and available computational resources.

These strategies demonstrate how freezing can be effectively used to make fine-tuning large pre-trained models tractable, even with limited computational resources. The best approach depends on the specific requirements of the task, the similarity between the source and target domains, and the available computational budget.